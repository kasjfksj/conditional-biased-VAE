{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torchsummary import summary\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 100\n",
    "# MNIST Dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (Input Pipeline)\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnFlatten(nn.Module):\n",
    "    def forward(self, input, size=576):\n",
    "        return input.view(input.size(0), size, 1, 1)\n",
    "    \n",
    "# class VariationalAutoEncoderCNN(nn.Module):\n",
    "#     def __init__(self, z_dim, h_dim=576):\n",
    "#         super().__init__()\n",
    "#         # encoder\n",
    "#         self.encoder = nn.Sequential(\n",
    "#             nn.Conv2d(3, 64, kernel_size=3),\n",
    "#             nn.MaxPool2d(kernel_size=2),\n",
    "#             nn.Conv2d(64, 16, kernel_size=3),\n",
    "#             nn.MaxPool2d(kernel_size=2),\n",
    "#             nn.Flatten()\n",
    "#         )\n",
    "        \n",
    "#         self.fc1 = nn.Linear(h_dim, 256)\n",
    "#         self.fc2 = nn.Linear(256,z_dim)\n",
    "#         self.fc3 = nn.Linear(256, z_dim)\n",
    "#         self.fc4 = nn.Linear(z_dim, 256)\n",
    "#         self.fc5 = nn.Linear(256,h_dim)\n",
    "        \n",
    "#         self.decoder = nn.Sequential(\n",
    "#             UnFlatten(),\n",
    "#             nn.ConvTranspose2d(h_dim, 16, kernel_size=3, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(16, 32, kernel_size=3, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(32, 64, kernel_size=3, stride=2),\n",
    "#             nn.ReLU(),\n",
    "#             nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2),\n",
    "#         )\n",
    "\n",
    "#     def encode(self, x):\n",
    "#         h = self.encoder(x)\n",
    "#         h=self.fc1(h)\n",
    "#         mu, sigma = self.fc2(h), self.fc3(h)\n",
    "#         return mu, sigma\n",
    "\n",
    "#     def decode(self, z):\n",
    "#         z = self.fc4(z)\n",
    "\n",
    "#         z = self.fc5(z)\n",
    "#         z = self.decoder(z)\n",
    "#         return z\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mu, sigma = self.encode(x)\n",
    "\n",
    "#         # Sample from latent distribution from encoder\n",
    "#         epsilon = torch.randn_like(sigma)\n",
    "#         z_reparametrized = mu + sigma*epsilon\n",
    "\n",
    "#         x = self.decode(z_reparametrized)\n",
    "#         return x, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoderMLP(nn.Module):\n",
    "    def __init__(self,z_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256,z_dim)\n",
    "        self.fc3 = nn.Linear(256, z_dim)\n",
    "        self.fc4 = nn.Linear(z_dim, 256)\n",
    "        self.fc5 = nn.Linear(256,784)\n",
    "        \n",
    "        \n",
    "    def encode(self,x):\n",
    "        x=self.fc1(x)\n",
    "        return self.fc2(x),self.fc3(x)\n",
    "    def decode(self,x):\n",
    "        x = self.fc4(x)\n",
    "        x = self.fc5(x)\n",
    "        return F.sigmoid(x)\n",
    "    def forward(self, x):\n",
    "        mu, log_sigma = self.encode(x)\n",
    "        sigma = torch.exp(log_sigma*0.5)\n",
    "        epsilon = torch.randn_like(sigma)\n",
    "        z_reparametrized = mu + sigma*epsilon\n",
    "\n",
    "        x = self.decode(z_reparametrized)\n",
    "        return x, mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "INPUT_DIM = 784\n",
    "NUM_EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "LR_RATE = 4e-4\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.detach().numpy()\n",
    "    t = np.transpose(npimg, (1, 2, 0))\n",
    "    plt.imshow(t)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12596.353515625\n",
      "10272.1337890625\n",
      "9770.2138671875\n",
      "10231.0283203125\n",
      "10397.974609375\n",
      "10244.765625\n",
      "9362.0966796875\n",
      "9908.1357421875\n",
      "10182.73046875\n",
      "10014.92578125\n",
      "9846.6884765625\n",
      "10458.9921875\n",
      "9696.26953125\n"
     ]
    }
   ],
   "source": [
    "def train(num_epochs, model, optimizer):\n",
    "    # Start training\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_idx, (x, _) in enumerate(train_loader):\n",
    "        \n",
    "            # Forward pass\n",
    "            # print(x[0][0])\n",
    "            # imshow(torchvision.utils.make_grid(x[1]))\n",
    "            x=x[:,0,:,:].view(-1,784).to(device)\n",
    "            x_reconst, mu, log_sigma = model(x)\n",
    "            sigma = torch.exp(log_sigma)\n",
    "            # loss, formulas from https://www.youtube.com/watch?v=igP03FXZqgo&t=2182s\n",
    "            reconst_loss = F.binary_cross_entropy(x_reconst, x.view(-1, 784), reduction='sum')    #First part of ELBO, reconstruction loss, measures how good is reconstructed image\n",
    "            kl_div = - torch.sum(1 + torch.log(sigma.pow(2)) - mu.pow(2) - sigma.pow(2)) #Second part of ELBO, measure difference between distribution of latent space and normal distribution\n",
    "\n",
    "            # Backprop and optimize\n",
    "            loss = reconst_loss + kl_div\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch%4==0:\n",
    "            print(loss.item())\n",
    "# summary(model,(3,32,32))\n",
    "# Run training\n",
    "model=VariationalAutoEncoderMLP(z_dim=16).to(device)\n",
    "train(\n",
    "    NUM_EPOCHS,\n",
    "    model,\n",
    "    torch.optim.Adam(model.parameters(), lr=LR_RATE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m     x_reconst \u001b[38;5;241m=\u001b[39m x_reconst\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m)\n\u001b[1;32m     44\u001b[0m     imshow(torchvision\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mmake_grid(x_reconst))\n\u001b[0;32m---> 46\u001b[0m \u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_examples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m, in \u001b[0;36minference\u001b[0;34m(digit, num_examples)\u001b[0m\n\u001b[1;32m     23\u001b[0m imshow(torchvision\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mmake_grid(d[\u001b[38;5;241m1\u001b[39m]))\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# mu, sigma = model.encode(images[d].view(-1,3,32,32))\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m mu, log_sigma \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(mu)\n\u001b[1;32m     27\u001b[0m sigma \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(log_sigma)\n",
      "Cell \u001b[0;32mIn[6], line 12\u001b[0m, in \u001b[0;36mVariationalAutoEncoderMLP.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[0;32m---> 12\u001b[0m     x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x),\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/miniconda3/envs/3.12/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "loop = tqdm(enumerate(train_loader))\n",
    "def inference(digit, num_examples=1):\n",
    "    \"\"\"\n",
    "    Generates (num_examples) of a particular digit.\n",
    "    Specifically we extract an example of each digit,\n",
    "    then after we have the mu, sigma representation for\n",
    "    each digit we can sample from that.\n",
    "\n",
    "    After we sample we can run the decoder part of the VAE\n",
    "    and generate examples.\n",
    "    \"\"\"\n",
    "    from_original = []\n",
    "    images =[]\n",
    "    idx = 0\n",
    "    for x, y in train_loader:\n",
    "        idx+=1\n",
    "        images.append(x)\n",
    "        if idx==3:\n",
    "            break\n",
    "    encodings_digit = []\n",
    "    for d in images:\n",
    "        with torch.no_grad():\n",
    "            imshow(torchvision.utils.make_grid(d[1]))\n",
    "            # mu, sigma = model.encode(images[d].view(-1,3,32,32))\n",
    "            mu, log_sigma = model.encode(d[1].view(-1,784))\n",
    "            sigma = torch.exp(log_sigma)\n",
    "            epsilon = torch.randn_like(torch.exp(sigma))\n",
    "            z_reparametrized = mu + sigma*epsilon\n",
    "            x_reconst = model.decode(z_reparametrized)\n",
    "            x_reconst = x_reconst.view(-1,28,28)\n",
    "            from_original.append(x_reconst)\n",
    "            imshow(torchvision.utils.make_grid(x_reconst))\n",
    "        encodings_digit.append((mu, sigma))\n",
    "        # save_image(out, f\"generated_{digit}_ex{example}.png\")\n",
    "    \n",
    "    sigma = torch.rand(16)*2-1\n",
    "    sigma = sigma.view(-1,16)\n",
    "    mu = torch.normal(0,1,size=(1,16))\n",
    "    epsilon = torch.randn_like(sigma)\n",
    "    z_reparametrized = mu\n",
    "    x_reconst = model.decode(z_reparametrized)\n",
    "    x_reconst = x_reconst.view(-1,28,28)\n",
    "    imshow(torchvision.utils.make_grid(x_reconst))\n",
    "\n",
    "inference(4,num_examples = 5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
